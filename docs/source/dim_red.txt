.. currentmodule:: mlpy

Dimensionality Reduction
========================

Linear Discriminant Analysis (LDA)
----------------------------------

.. autofunction:: lda


Fast Linear Discriminant Analysis
---------------------------------
Fast LDA implementation described in [Cai08]_.

.. autofunction:: fastlda


Principal Component Analysis (PCA)
----------------------------------

.. autofunction:: pca


Kernel Principal Component Analysis (KPCA)
------------------------------------------

.. autofunction:: kpca

Example:

>>> import numpy as np
>>> import matplotlib.pyplot as plt
>>> import mlpy
>>> np.random.seed(0)
>>> x = np.zeros((150, 2))
>>> y = np.empty(150, dtype=np.int)
>>> theta = np.random.normal(0, np.pi, 50)
>>> r = np.random.normal(0, 0.1, 50)
>>> x[0:50, 0] = r * np.cos(theta)
>>> x[0:50, 1] = r * np.sin(theta)
>>> y[0:50] = 0
>>> theta = np.random.normal(0, np.pi, 50)
>>> r = np.random.normal(2, 0.1, 50)
>>> x[50:100, 0] = r * np.cos(theta)
>>> x[50:100, 1] = r * np.sin(theta)
>>> y[50:100] = 1
>>> theta = np.random.normal(0, np.pi, 50)
>>> r = np.random.normal(5, 0.1, 50)
>>> x[100:150, 0] = r * np.cos(theta)
>>> x[100:150, 1] = r * np.sin(theta)
>>> y[100:150] = 2
>>> cmap = plt.set_cmap(plt.cm.Paired)
>>> gK = mlpy.kernel_gaussian(x, x, sigma=2) # gaussian kernel matrix
>>> pK = mlpy.kernel_polynomial(x, x, gamma=1.0, b=1.0, d=2.0) # polynomial kernel matrix
>>> gK = mlpy.kernel_center(gK, gK) # centers the kernel matrix
>>> pK = mlpy.kernel_center(pK, pK)
>>> gcoeff, gevals = mlpy.kpca(gK)
>>> pcoeff, pevals = mlpy.kpca(pK)
>>> gcoeff = gcoeff[:, :2] # firsts two principal components
>>> pcoeff = pcoeff[:, :2] # firsts two principal components
>>> gz = np.dot(gK, gcoeff)
>>> pz = np.dot(pK, pcoeff)
>>> fig = plt.figure(1)
>>> ax1 = plt.subplot(131)
>>> plot1 = plt.scatter(x[:, 0], x[:, 1], c=y)
>>> title1 = ax1.set_title('Original X')
>>> ax2 = plt.subplot(132)
>>> plot2 = plt.scatter(gz[:, 0], gz[:, 1], c=y)
>>> title2 = ax2.set_title('Gaussian kernel')
>>> ax3 = plt.subplot(133)
>>> plot3 = plt.scatter(pz[:, 0], pz[:, 1], c=y)
>>> title3 = ax3.set_title('Polynomial kernel')
>>> plt.show()

.. image:: images/kernel_pca.png

Fast Principal Component Analysis (FastPCA)
-------------------------------------------
Fast PCA implementation described in [Sharma07]_.

.. autofunction:: fastpca

Example reproducing Figure 1 of [Sharma07]_:

>>> import numpy as np
>>> import matplotlib.pyplot as plt
>>> import mlpy
>>> np.random.seed(0)
>>> h = 10 # dimension reduced to h=10
>>> n = 100 # number of samples
>>> d = np.array([100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000]) # number of dimensions
>>> mse_eig, mse_fast = np.zeros(len(d)), np.zeros(len(d))
>>> for i in range(d.shape[0]):
...     xorig = np.random.rand(100, d[i])
...     xmean = np.mean(xorig, axis=0) # compute the columns mean
...     xc = xorig - xmean # center the data
...     evecs_eig, _ = mlpy.pca(xc) # pca (eigenvalue decomposition)
...     evecs_eig = evecs_eig[:, :h] # keep the first h dimensions
...     evecs_fast = mlpy.fastpca(xc, h, eps=0.01) # fast pca
...     y_eig = np.dot(xc, evecs_eig) # reduced dimensional feature vectors (eigenvalue decomposition)
...     y_fast = np.dot(xc, evecs_fast)  # reduced dimensional feature vectors (fast pca)
...     xhat_eig = np.dot(y_eig, evecs_eig.T) + xmean # reconstructed vector (eigenvalue decomposition)
...     xhat_fast = np.dot(y_fast, evecs_fast.T) + xmean # reconstructed vector (fast pca)
...
>>> for j in range(n):
...     mse_eig[i] += np.sum((xorig[j] - xhat_eig[j])**2)
...     mse_fast[i] += np.sum((xorig[j] - xhat_fast[j])**2)
...
>>> mse_eig[i] /= n
>>> mse_fast[i] /= n
>>> fig = plt.figure(1)
>>> plot1 = plt.plot(d, mse_eig, '|-b', label="PCA using eigenvalue decomposition")
>>> plot2 = plt.plot(d, mse_fast, '.-g', label="Fast PCA")
>>> leg = plt.legend(loc = 'best')
>>> xl = plt.xlabel("Data dimensionality")
>>> yl = plt.ylabel("Mean Squared Error")
>>> plt.show()

.. image:: images/fastpca.png


Spectral Regression Discriminant Analysis (SRDA)
------------------------------------------------

.. autofunction:: srda

Example:
	
>>> import numpy as np
>>> import matplotlib.pyplot as plt
>>> import mlpy
>>> mean1, cov1, n1 = [1, 5], [[1,1],[1,2]], 200  # 200 samples of class 0
>>> x1 = np.random.multivariate_normal(mean1, cov1, n1)
>>> y1 = np.zeros(n1, dtype=np.int)
>>> mean2, cov2, n2 = [2.5, 2.5], [[1,0],[0,1]], 300 # 300 samples of class 1
>>> x2 = np.random.multivariate_normal(mean2, cov2, n2)
>>> y2 = np.ones(n2, dtype=np.int)
>>> mean3, cov3, n3 = [5, 8], [[0.5,0],[0,0.5]], 200 # 200 samples of class 2
>>> x3 = np.random.multivariate_normal(mean3, cov3, n3)
>>> y3 = 2 * np.ones(n3, dtype=np.int)
>>> x = np.concatenate((x1, x2, x3), axis=0) # concatenate the samples
>>> y = np.concatenate((y1, y2, y3))
>>> x -= np.mean(x, axis=0) # center the data
>>> A = mlpy.srda(x, y, alpha=1) # compute the tranformation matrix
>>> A.shape
(2, 3)
>>> z = np.dot(x, A.T) # embed x to C-1=2 -dimensional subspace
>>> z.shape
(700, 2)
>>> plt.set_cmap(plt.cm.Paired)
>>> fig = plt.figure(1)
>>> plot1 = plt.scatter(x[:, 0], x[:, 1], c=y) # plot x
>>> plt.show()

.. image:: images/srda1.png

>>> fig = plt.figure(2)
>>> plot2 = plt.scatter(z[:, 0], z[:, 1], c=y) # plot z
>>> plt.show()

.. image:: images/srda2.png



.. [Cai08] D Cai, X He, J Han. SRDA: An Efficient Algorithm for Large-Scale Discriminant Analysis. Knowledge and Data Engineering, IEEE Transactions on Volume 20, Issue 1, Jan. 2008 Page(s):1 - 12.
.. [Sharma07]  A Sharma, K K Paliwal. Fast principal component analysis using fixed-point algorithm. Pattern Recognition Letters 28 (2007) 1151â€“1155.
